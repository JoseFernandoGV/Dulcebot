"""
dulcebot.py  â€“  ChainlitÂ +Â LangGraphÂ +Â Gemini
Ordenado, comentado y sin imports redundantes.
No se ha tocado el prompt original ni la lÃ³gica principal.
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1.  Imports y entorno
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
import os
import re
import json
from typing import Annotated, Literal

import chainlit as cl
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.schema.runnable.config import RunnableConfig

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages, MessagesState
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from pack import*
import os

# claves (en prod, usa variables de entorno seguras)
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "false"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2.  Tools y modelo LLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TOOLS = [
    consultar_faq,
    consultar_stock_producto,
    listar_productos,
    describir_producto,
]

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.4,
    top_p=0.9,
).bind_tools(TOOLS)  # se mantiene bind_tools para no alterar comportamiento

final_llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.4,
    top_p=0.95,
)

tool_node = ToolNode(tools=TOOLS)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3.  Prompt del sistema (sin cambios)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SYSTEM_PROMPT = SystemMessage(content="""
Eres **DulceBot**, asistente virtual de **DulceTentaciÃ³nÂ S.A.S.**
Tu personalidad: cercana, entusiasta, un punto juguetona â€” pero profesional. Siempre en espaÃ±ol.

GuÃ­as operativas:
â€¢ Si la duda coincide con una FAQ â†’ llama a consultar_faq (retorna una string).
â€¢ Si el usuario menciona un producto concreto â†’ llama a consultar_stock_producto (retorna una string).
â€¢ Si pide catÃ¡logo, productos o sabores generales â†’ llama a listar_productos (retorna un objeto JSON con {"tipo": "catÃ¡logo", "productos": [...]}).
â€¢ Si el usuario quiere detalles de un producto â†’ llama a describir_producto (retorna un objeto JSON con {"tipo": "detalle_producto", "producto": {...}}).

Siempre debes:
â€¢ Analizar si debes llamar una herramienta.
â€¢ Usar los datos tal como vienen, sin inventar.
â€¢ Incorporar el resultado en la respuesta con Markdown si es Ãºtil.
â€¢ Cerrar con: Â«Â¿Te ayudo en algo mÃ¡s?Â»
â€¢ Si la herramienta devuelve un mensaje con âš ï¸ o âŒ, dÃ­selo al usuario de forma amable.
â€¢ Usa emojis moderados (ğŸ°, ğŸ˜Š, âœ…, âš ï¸).
â€¢ Siempre trata de mostrar todo de manera decente, bonita y no en formato json.
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4.  Estado y nodo agent
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class State(MessagesState):
    messages: Annotated[list, add_messages]

def call_model(state: State):
    """Nodo agent: llama a Gemini con historial + prompt del sistema."""
    history = state["messages"]
    ai_msg = llm.invoke([SYSTEM_PROMPT] + history)
    return {"messages": [ai_msg]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5.  Helpers de formato y cordialidad
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _catalogo_a_tabla(productos: list[dict]) -> str:
    header = "| Producto | Precio |\n|----------|--------|\n"
    rows = "\n".join(f"| {p['nombre']} | ${p['precio']:,.0f} |" for p in productos)
    return header + rows

def _rewrite_as_cordial(msg: AIMessage):
    """Reâ€‘escribe la respuesta anterior en tono cordial (Ãºltimo recurso)."""
    if not msg.content or not msg.content.strip():
        msg.content = (
            "âš ï¸Â Lo siento, no pude generar una respuesta. "
            "Â¿PodrÃ­as reformular tu pregunta?"
        )
        return {"messages": [msg]}

    instr = (
        "Reescribe la respuesta anterior con un tono cordial y amable, "
        "usa Markdown si es Ãºtil y finaliza con Â«Â¿Te ayudo en algo mÃ¡s?Â». "
        "No inventes datos nuevos."
    )
    new_msg = final_llm.invoke([SYSTEM_PROMPT, msg, HumanMessage(content=instr)])
    new_msg.id = msg.id
    return {"messages": [new_msg]}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6.  Nodo final: formatea la salida de las tools
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def call_final_model(state: State):
    last_ai: AIMessage = state["messages"][-1]

    # Si no viene de una tool, solo reâ€‘escribimos si hace falta
    if not getattr(last_ai, "tool_calls", None):
        return _rewrite_as_cordial(last_ai)

    data = last_ai.additional_kwargs.get("tool_output") or {}
    if not data:
        try:
            data = json.loads(re.sub(r"```json|```", "", last_ai.content))
        except Exception:
            pass

    tipo = data.get("tipo")

    if tipo == "faq":
        respuesta = data.get("respuesta") or data.get("mensaje")
        last_ai.content = f"ğŸ’¬ {respuesta}\n\nÂ¿Te ayudo en algo mÃ¡s?"
        return {"messages": [last_ai]}

    if tipo == "catÃ¡logo":
        prods = data.get("productos", [])
        last_ai.content = (
            "âš ï¸Â Actualmente no hay productos disponibles."
            if not prods
            else (
                "Â¡Con gusto! Este es nuestro catÃ¡logo actual ğŸ°:\n\n"
                f"{_catalogo_a_tabla(prods)}\n\nÂ¿Te ayudo en algo mÃ¡s?"
            )
        )
        return {"messages": [last_ai]}

    if tipo == "detalle_producto":
        if not data.get("encontrado", False):
            last_ai.content = data.get("mensaje", "âŒÂ Producto no encontrado.")
        else:
            p = data["producto"]
            last_ai.content = (
                "ğŸ° **Detalles del producto**\n\n"
                f"**{p['nombre']}** â€” {p['descripcion']}\n"
                f"Precio: ${p['precio']:,.0f}Â COP\n"
                f"Stock: {p['stock']} unidades\n\nÂ¿Te ayudo en algo mÃ¡s?"
            )
        return {"messages": [last_ai]}

    if tipo == "stock":
        texto = data.get("nombre") and (
            f"âœ…Â El producto '{data['nombre']}' estÃ¡ disponible por "
            f"${data['precio']:,.0f}Â COP (stock {data['stock']} unidades)."
        ) or data.get("mensaje")
        last_ai.content = f"{texto}\n\nÂ¿Te ayudo en algo mÃ¡s?"
        return {"messages": [last_ai]}

    return _rewrite_as_cordial(last_ai)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7.  LÃ³gica para decidir si pasar por tools
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
KEYWORDS_STOCK = [
    "stock", "disponible", "productos", "catÃ¡logo", "precio", "precios",
    "sabores", "sabor", "descrÃ­beme", "describe",
]
KEYWORDS_FAQ = [
    "formas de pago", "pago", "pagos", "horario", "envÃ­os",
    "domicilio", "dÃ³nde estÃ¡n", "ubicados", "sin azÃºcar",
    "pedido", "cÃ³mo pido",
]

def should_continue(state: State) -> Literal["tools", "final"]:
    """HeurÃ­stica sencilla para decidir si invocar herramientas."""
    last = state["messages"][-1]

    # Gemini solicitÃ³ tool
    if isinstance(last, AIMessage) and getattr(last, "tool_calls", None):
        return "tools"

    # Ya es ToolMessage
    if hasattr(last, "name") and getattr(last, "tool_call_id", None):
        return "tools"

    # Reglas por palabras clave
    if isinstance(last, HumanMessage):
        txt = last.content.lower()
        if re.search(r"\b(describe|descrÃ­beme|descrÃ­belo|descrÃ­bela)\b", txt):
            return "tools"
        if any(k in txt for k in KEYWORDS_STOCK + KEYWORDS_FAQ):
            return "tools"

    return "final"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8.  ConstrucciÃ³n del grafo
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
builder = StateGraph(State)
builder.add_node("agent", call_model)
builder.add_node("tools", tool_node)
builder.add_node("final", call_final_model)

builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", should_continue)
builder.add_edge("tools", "agent")  #Â mantiene flujo original
builder.add_edge("final", END)

graph = builder.compile(checkpointer=MemorySaver())

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9.  Chainlit
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@cl.on_chat_start
async def on_chat_start():
    await cl.Message(content="""
        âœ¨ Hola, soy **DulceBot**, tu asistente de **DulceTentaciÃ³n S.A.S.**

        Estoy aquÃ­ para ayudarte con preguntas sobre nuestros productos, formas de pago, tiempos de entrega o sabores disponibles.

        Puedes preguntarme cosas como:
        - Â¿QuÃ© sabores de cheesecake tienen hoy?
        - Â¿CuÃ¡nto cuesta un brownie?
        - Â¿Hacen entregas en Cali?
    """).send()

@cl.on_message
async def on_message(msg: cl.Message):
    cfg = {"configurable": {"thread_id": cl.context.session.id}}
    cb  = cl.LangchainCallbackHandler()
    out = cl.Message(content="")

    try:
        # 1ï¸âƒ£  intento rÃ¡pido con embeddings (FAQ)
        faq = evaluar_y_sumar_frecuencia_si_corresponde(msg.content)
        if faq:
            r = llm.invoke(
                [
                    SYSTEM_PROMPT,
                    HumanMessage(
                        content=(
                            f"InformaciÃ³n base: Â«{faq}Â»\n"
                            f"Pregunta: Â«{msg.content}Â»\n\n"
                            "Responde con calidez sin inventar datos."
                        )
                    ),
                ]
            )
            await cl.Message(r.content).send()
            return
        
        # 2ï¸âƒ£  Flujo LangGraph
        for resp, meta in graph.stream(
            {"messages": [HumanMessage(content=msg.content)]},
            stream_mode="messages",
            config=RunnableConfig(callbacks=[cb], **cfg),
        ):
            if (
                meta.get("langgraph_node") == "final"
                and resp.content
                and not isinstance(resp, HumanMessage)
            ):
                await out.stream_token(resp.content)
        await out.send()

    except Exception as err:
        error_msg = traceback.format_exc()
        db.registrar_error(error_msg)
        print("âŒ Error:", err)
        await cl.Message(
            "âŒ Lo siento, ocurriÃ³ un error. Intenta de nuevo o contacta soporte."
        ).send()
